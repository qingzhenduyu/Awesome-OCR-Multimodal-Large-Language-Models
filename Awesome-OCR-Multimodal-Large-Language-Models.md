### Awesome-OCR-Multimodal-Large-Language-Models

|Title|Date|Publication|Code|Paper|
|---|---|---|---|---|
|**[On the Hidden Mystery of OCR in Large Multimodal Models](https://arxiv.org/abs/2305.07895)**|2023.05||[Github](https://github.com/Yuliang-Liu/MultimodalOCR)|[Paper](https://arxiv.org/abs/2305.07895)|
|**[mPLUG-DocOwl: Modularized Multimodal Large Language Model for Document Understanding](https://arxiv.org/abs/2307.02499)**|2023.07||[Github](https://github.com/X-PLUG/mPLUG-DocOwl)|[Paper](https://arxiv.org/abs/2307.02499)|
|**[Nougat: Neural Optical Understanding for Academic Documents](https://arxiv.org/abs/2308.13418)**|2023.08||[Github](https://github.com/facebookresearch/nougat)|[Paper](https://arxiv.org/abs/2308.13418)|
|**[UReader: Universal OCR-free Visually-situated Language Understanding with Multimodal Large Language Model](https://arxiv.org/abs/2310.05126)**|2023.10||[Github](https://github.com/LukeForeverYoung/UReader)|[Paper](https://arxiv.org/abs/2310.05126)|
|**[Monkey: Image Resolution and Text Label Are Important Things for Large Multi-modal Models](https://arxiv.org/abs/2311.06607)**|2023.11|CVPR 2024 Highlight|[Github](https://github.com/Yuliang-Liu/Monkey)|[Paper](https://arxiv.org/abs/2311.06607)|
|**[Vary: Scaling up the Vision Vocabulary for Large Vision-Language Models](https://arxiv.org/abs/2312.06109)**|2023.12|ECCV 2024|[Github](https://github.com/Ucas-HaoranWei/Vary)|[Paper](https://arxiv.org/abs/2312.06109)|
|**[LOCR: Location-Guided Transformer for Optical Character Recognition](https://arxiv.org/abs/2403.02127)**|2024.03|ACL 2024 Findings|-|[Paper](https://arxiv.org/abs/2403.02127)|
|**[TextMonkey: An OCR-Free Large Multimodal Model for Understanding Document](https://arxiv.org/abs/2403.04473)**|2024.03||[Github](https://github.com/Yuliang-Liu/Monkey)|[Paper](https://arxiv.org/abs/2403.04473)|
|**[How Far Are We to GPT-4V? Closing the Gap to Commercial Multimodal Models with Open-Source Suites](https://arxiv.org/abs/2404.16821)**|2024.04||[Github](https://github.com/OpenGVLab/InternVL)|[Paper](https://arxiv.org/abs/2404.16821)|
|**[TextHawk: Exploring Efficient Fine-Grained Perception of Multimodal Large Language Models](https://arxiv.org/abs/2404.09204)**|2024.04||[Github](https://github.com/yuyq96/TextHawk)|[Paper](https://arxiv.org/abs/2404.09204)|
|**[General OCR Theory: Towards OCR-2.0 via a Unified End-to-end Model](https://arxiv.org/abs/2409.01704)**|2024.09||[Github](https://github.com/Ucas-HaoranWei/GOT-OCR2.0)|[Paper](https://arxiv.org/abs/2409.01704)|
|**[TextHawk2: A Large Vision-Language Model Excels in Bilingual OCR and Grounding with 16x Fewer Tokens](https://arxiv.org/abs/2410.05261)**|2024.10||-|[Paper](https://arxiv.org/abs/2410.05261)|

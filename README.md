### Awesome-OCR-Multimodal-Large-Language-Models

|Title|Date|Institution|Publication|Code|Paper|
|:---:|:---:|:---:|:---:|:---:|:---:|
| **[OCR-free Document Understanding Transformer](https://arxiv.org/abs/2111.15664)**                                                               | 2021.11 | NAVER                                             | ECCV 2022           | [Github](https://github.com/clovaai/donut)                         | [Paper](https://arxiv.org/abs/2111.15664)     |
| **[On the Hidden Mystery of OCR in Large Multimodal Models](https://arxiv.org/abs/2305.07895)**                                                   | 2023.05 | Huazhong University of Science and Technology     |                     | [Github](https://github.com/Yuliang-Liu/MultimodalOCR)             | [Paper](https://arxiv.org/abs/2305.07895)     |
| **[mPLUG-DocOwl: Modularized Multimodal Large Language Model for Document Understanding](https://arxiv.org/abs/2307.02499)**                      | 2023.07 | Alibaba Group                                     |                     | [Github](https://github.com/X-PLUG/mPLUG-DocOwl)                   | [Paper](https://arxiv.org/abs/2307.02499)     |
| **[Nougat: Neural Optical Understanding for Academic Documents](https://arxiv.org/abs/2308.13418)**                                               | 2023.08 | Meta AI                                           |                     | [Github](https://github.com/facebookresearch/nougat)               | [Paper](https://arxiv.org/abs/2308.13418)     |
| **[UReader: Universal OCR-free Visually-situated Language Understanding with Multimodal Large Language Model](https://arxiv.org/abs/2310.05126)** | 2023.10 | East China Normal University<br><br>Alibaba Group | EMNLP 2023          | [Github](https://github.com/LukeForeverYoung/UReader)              | [Paper](https://arxiv.org/abs/2310.05126)     |
| **[mPLUG-PaperOwl: Scientific Diagram Analysis with the Multimodal Large Language Model](https://arxiv.org/abs/2311.18248)**                      | 2023.11 | Alibaba Group                                     | ACM MM 2024         | [Github](https://github.com/X-PLUG/mPLUG-DocOwl)                   | [Paper](https://arxiv.org/abs/2311.18248)     |
| **[Monkey: Image Resolution and Text Label Are Important Things for Large Multi-modal Models](https://arxiv.org/abs/2311.06607)**                 | 2023.11 | Huazhong University of Science and Technology     | CVPR 2024 Highlight | [Github](https://github.com/Yuliang-Liu/Monkey)                    | [Paper](https://arxiv.org/abs/2311.06607)     |
| **[Vary: Scaling up the Vision Vocabulary for Large Vision-Language Models](https://arxiv.org/abs/2312.06109)**                                   | 2023.12 | MEGVII                                            | ECCV 2024           | [Github](https://github.com/Ucas-HaoranWei/Vary)                   | [Paper](https://arxiv.org/abs/2312.06109)     |
| **[LOCR: Location-Guided Transformer for Optical Character Recognition](https://arxiv.org/abs/2403.02127)**                                       | 2024.03 | Shanghai Artificial Intelligence Laboratory       | ACL 2024 Findings   | -                                                                  | [Paper](https://arxiv.org/abs/2403.02127)     |
| **[mPLUG-DocOwl 1.5: Unified Structure Learning for OCR-free Document Understanding](https://arxiv.org/abs/2403.12895)**                          | 2024.03 | Alibaba Group                                     | EMNLP 2024          | [Github](https://github.com/X-PLUG/mPLUG-DocOwl)                   | [Paper](https://arxiv.org/abs/2403.12895)     |
| **[TextMonkey: An OCR-Free Large Multimodal Model for Understanding Document](https://arxiv.org/abs/2403.04473)**                                 | 2024.03 | Huazhong University of Science and Technology     |                     | [Github](https://github.com/Yuliang-Liu/Monkey)                    | [Paper](https://arxiv.org/abs/2403.04473)     |
| **[How Far Are We to GPT-4V? Closing the Gap to Commercial Multimodal Models with Open-Source Suites](https://arxiv.org/abs/2404.16821)**         | 2024.04 | Shanghai AI Laboratory                            |                     | [Github](https://github.com/OpenGVLab/InternVL)                    | [Paper](https://arxiv.org/abs/2404.16821)     |
| **[TextHawk: Exploring Efficient Fine-Grained Perception of Multimodal Large Language Models](https://arxiv.org/abs/2404.09204)**                 | 2024.04 | Huawei                                            |                     | [Github](https://github.com/yuyq96/TextHawk)                       | [Paper](https://arxiv.org/abs/2404.09204)     |
| **[Focus Anywhere for Fine-grained Multi-page Document Understanding](https://arxiv.org/abs/2405.14295)**                                         | 2024.05 | UCAS<br><br>MEGVII                                |                     | [Github](https://github.com/ucaslcl/Fox)                           | [Paper](https://arxiv.org/abs/2405.14295)     |
| **[General OCR Theory: Towards OCR-2.0 via a Unified End-to-end Model](https://arxiv.org/abs/2409.01704)**                                        | 2024.09 | StepFun<br><br>UCAS                               |                     | [Github](https://github.com/Ucas-HaoranWei/GOT-OCR2.0)             | [Paper](https://arxiv.org/abs/2409.01704)     |
| **[mPLUG-DocOwl2: High-resolution Compressing for OCR-free Multi-page Document Understanding](https://www.arxiv.org/abs/2409.03420)**             | 2024.09 | Alibaba Group                                     |                     | [Github](https://github.com/X-PLUG/mPLUG-DocOwl/tree/main/DocOwl2) | [Paper](https://www.arxiv.org/abs/2409.03420) |
| **[TextHawk2: A Large Vision-Language Model Excels in Bilingual OCR and Grounding with 16x Fewer Tokens](https://arxiv.org/abs/2410.05261)**      | 2024.10 | Huawei                                            |                     | -                                                                  | [Paper](https://arxiv.org/abs/2410.05261)     |

